import networkx as nx
import numpy as np
import pandas as pd
from cvxopt import blas, solvers
import cvxopt as opt
from trees import importdata
from sklearn.covariance import ledoit_wolf
import progressbar


def weighted_degree_centrality(T):
    """Return a dictionary of weighted degree centrality for each node,
    the weighted degree is defined as (2-gower weight)"""
    # for u, v, d in T.edges(data=True):
    #     d['weight2'] = 2 - d['weight']
    degree = nx.degree(T, weight='weight+1')
    degree = dict(degree)
    degree_c = {}
    total = T.size(weight='weight+1')
    for i in degree:
        degree_c[i] = degree[i] / total
    return degree_c


def port_change(list1, list2):
    """Calculate the percentage change between two stock lists"""
    count = 0
    if not list1:
        return None
    else:
        for x in list2:
            if x not in list1:
                count += 1.0
    return count / len(list2)


def get_portfolios(dic, c_measure, quantile=0.25):
    """dic: dictionary of MST, which can be generated by function MST()
    returns a dictionary of central and peripheral stock list for each window, the key is enddate of the window.
    apply get_portfolios function to c_measure in ['degree','closeness','betweenness']
    to get three dictionaries for these three measure of centrality."""
    result = {}
    for k, T in dic.items():
        subresult = {}
        central = portfolio(T, c_measure, quantile, "upper")
        peripheral = portfolio(T, c_measure, quantile, "lower")
        subresult["central"] = central
        subresult["peripheral"] = peripheral
        result[k] = subresult
    return result


def portfolio_change(dic):
    """dic: dictonary of central and peripheral stock list for each window, generated from get_portfolio() function
    returns a dataframe of percentage change for stock list"""
    dates = sorted(dic.keys())
    change_dict_c = {}
    change_dict_p = {}
    for i in range(0, len(dates) - 1):
        list1c = dic[dates[i]]["central"]
        list2c = dic[dates[i + 1]]["central"]
        list1p = dic[dates[i]]["peripheral"]
        list2p = dic[dates[i + 1]]["peripheral"]
        change_dict_c[dates[i + 1]] = port_change(list1c, list2c)
        change_dict_p[dates[i + 1]] = port_change(list1p, list2p)
    c = pd.DataFrame(data=change_dict_c, index=["Central"]).T
    p = pd.DataFrame(data=change_dict_p, index=["Peripheral"]).T
    change = pd.concat([c, p], axis=1, join='inner')
    return change


def measure_performance(pricedf, stocklist, startdate, space=1, weights=None):
    start = int(np.where(pricedf.index == startdate)[0])
    end = start + space
    try:
        df = pricedf.iloc[[start, end]][stocklist]
        date = sorted(df.index)[1]
        p_current = df.iloc[0]
        p_next = df.iloc[1]
        a = (p_next / p_current).values
        r = np.average(a, weights=weights)
        return date, r
    except:
        return None, None


def performance(dic, pricedf, space=1, window=100, weights=False, shrinkage='None'):
    """dic: dictonary of central and peripheral stock list for each window, generated from get_portfolio() function"""
    colnames = ["Upper", "Lower"]
    if weights:
        colnames.extend([str(x) + '_Weighted' for x in colnames])
        ret = pricedf / pricedf.shift(1)
        ret = ret.iloc[1:]
    colnames.sort()
    dates = sorted(dic.keys())
    performance = pd.DataFrame(columns=colnames)
    for d in dates:
        clist = dic[d]["central"]
        plist = dic[d]["peripheral"]
        date, cr = measure_performance(pricedf, clist, d, space, weights=None)
        pr = measure_performance(pricedf, plist, d, space, weights=None)[1]
        if date:
            performance.set_value(date, "Upper", cr)
            performance.set_value(date, "Lower", pr)
        if weights:
            date, cr = measure_performance(pricedf, clist, d, space, weights=None)
            pr = measure_performance(pricedf, plist, d, space, weights=None)[1]
            performance.set_value(date, "Upper", cr)
            performance.set_value(date, "Lower", pr)
            # use cov_matrix() in clustering_functions.py
            c_cov = cov_matrix(ret, clist, window, date, shrinkage=shrinkage)
            p_cov = cov_matrix(ret, plist, window, date, shrinkage=shrinkage)
            cw = min_variance_weights(c_cov)
            pw = min_variance_weights(p_cov)
            crw = measure_performance(pricedf, clist, d, space, weights=cw)[1]
            prw = measure_performance(pricedf, plist, d, space, weights=pw)[1]
            if date:
                performance.set_value(date, "Upper_Weighted", crw)
                performance.set_value(date, "Lower_Weighted", prw)
    performance = performance.sort_index()
    return performance


def clustering_universe(trees, clusterings, c_measure, quantile=0.25):
    """compute the central and peripheral universes according to a given dict of {date: clustering}"""
    result = {}
    sorteddates = sorted(trees.keys(), key=lambda d: map(int, d.split('-')))
    bar = progressbar.ProgressBar(max_value=len(sorteddates))
    count = 0
    for k in sorteddates:
        T = trees[k]
        subresult = {}
        C = clusterings[k]
        peripheral = []
        central = []
        for c in C:
            if len(list(T.subgraph(c).edges())) == 0:
                # elements in clusters with no edges will be considered peripheral
                peripheral.extend(c)
            else:
                peripheral.extend(portfolio(T.subgraph(c), c_measure, quantile, "lower"))
                central.extend(portfolio(T.subgraph(c), c_measure, quantile, "upper"))
        subresult["central"] = central
        subresult["peripheral"] = peripheral
        result[k] = subresult
        count=count+1
        bar.update(count)
    return result


# include_all=False: pick one if there are several stocks with the same highest centrality
def portfolio(MST, c_measure, quantile=0.25, option='upper',include_all=False):
    """Return a list of the upper or lower 25%(default) of stocks sorted by centrality.
    If the quantile is 0, would return the single one with the highest (lowest) centrality"""
    if c_measure == 'closeness':
        centrality = nx.closeness_centrality(MST, distance="length")
    elif c_measure == 'degree':
        centrality = weighted_degree_centrality(MST)
    elif c_measure == 'betweenness':
        centrality = nx.betweenness_centrality(MST, weight="weight+1")
    else:
        print('wrong centrality measure.')
        return None
    items = centrality.items()
    items.sort(key=lambda item: (item[1], item[0]))
    v = [item[1] for item in items]
    sorted_ts = [item[0] for item in items]
    number = int(len(sorted_ts) * quantile)
    if number == 0:
        number = 1
    if option == 'lower':
        pos = len(v) - v[::-1].index(v[number])
        stock_list = sorted_ts[:pos]
    elif option == 'upper':
        if v[-number] == 0 and max(v)>0:
            pos = len(v) - v[::-1].index(0)
        else:
            pos = v.index(v[-number])
        stock_list = sorted_ts[pos:]
    if include_all == False:
        stock_list = stock_list[-number:]
    else:
        print "option must be upper or lower"
        return None
    return stock_list


def no_cluster_universe(trees, c_measure, quantile=0.25):
    result = {}
    sorteddates = sorted(trees.keys(), key=lambda d: map(int, d.split('-')))
    for k in sorteddates:
        T = trees[k]
        subresult = {}
        peripheral = []
        central = []
        peripheral.extend(portfolio(T, c_measure, quantile, "lower"))
        central.extend(portfolio(T, c_measure, quantile, "upper"))
        subresult["central"] = central
        subresult["peripheral"] = peripheral
        result[k] = subresult
    return result


def cov_matrix(ret, stocklist, window=250, enddate="2017-02-28"): # shrinkage='None'):
    """To generate covariance matrix for a certain period and a list of stock (stocklist),
     differs from the one in 'all_functions.py' by the 'stocklist' argument"""
    end = int(np.where(pd.to_datetime(ret.index).strftime("%Y-%m-%d") == enddate)[0])
    start = end - window
    subret = ret[start:end][stocklist]
    # if shrinkage == "None":
    cov_mat = np.cov(subret.T)
    # elif shrinkage == "LedoitWolf":
        # cov_mat = ledoit_wolf(subret, assume_centered=True)[0]
    # else:
    #     print "'shrinkage' can only be 'None' or 'LedoitWolf'"
    #     return None
    return cov_mat


def min_variance_weights(cov):
    S = opt.matrix(cov)
    n = cov.shape[0]
    q = opt.matrix(0.0, (n, 1))
    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix
    h = opt.matrix(0.0, (n, 1))
    A = opt.matrix(1.0, (1, n))
    b = opt.matrix(1.0)
    solvers.options['show_progress'] = False
    weights = solvers.qp(S, q, G, h, A, b)["x"]
    risk = np.sqrt(blas.dot(weights, S * weights))
    return np.asarray(weights), risk

def markowitz(cov, mean_return=[], expected_return=0, risk_free_asset=True, risk_free_rate=0.02,
              allow_short_selling=False, long_cap=None, short_cap=None):
    S = opt.matrix(cov)
    n = cov.shape[0]
    if risk_free_asset:
        temp = np.diag(np.repeat(0.0,n+1))
        temp[:-1,:-1] = cov
        S = opt.matrix(temp)
        n=temp.shape[0]
    q = opt.matrix(0.0, (n, 1))
    A = opt.matrix(1.0, (1, n))
    b = opt.matrix(1.0)
    if not allow_short_selling:
        G = -np.eye(n)
        h = np.repeat(0.0,n)
    else:
        G = [np.repeat(0.0,n)]
        if risk_free_asset:
            G[0][-1] = -1# negative n x n identity matrix
        h = [0.0]
    if list(mean_return):
        if risk_free_asset:
            G = np.append(G,[np.append(-mean_return,-risk_free_rate)],axis=0)
        else:
            G = np.append(G,[-mean_return],axis=0)
        h = np.append(h, -expected_return)
    if long_cap != None:
        G = np.append(G,np.eye(n),axis=0)
        h = np.append(h,np.repeat(long_cap,n))
        if risk_free_asset:
            h = h[:-1]
            G = G[:-1]
    if short_cap != None:
        G = np.append(G,-np.eye(n),axis=0)
        h = np.append(h,np.repeat(short_cap,n))
        if risk_free_asset:
            h = h[:-1]
            G = G[:-1]
    G = opt.matrix(G)
    h = opt.matrix(h.T)
    solvers.options['show_progress'] = False
    weights = solvers.qp(S, q, G, h, A, b)["x"]
    risk = np.sqrt(blas.dot(weights, S * weights))
    return np.asarray(weights), risk

def clustering_performance(price, ret, div, cpr, universes, weighted=True, window=100):
    univdates = sorted(universes.keys(), key=lambda d: map(int, d.split('-')))
    pricedates = sorted(pd.to_datetime(price.index.values).strftime("%Y-%m-%d"), key=lambda d: map(int, d.split('-')))
    divdates = sorted(pd.to_datetime(div.index.values).strftime("%Y-%m-%d"), key=lambda d: map(int, d.split('-')))
    cprdates = sorted(pd.to_datetime(cpr.index.values).strftime("%Y-%m-%d"), key=lambda d: map(int, d.split('-')))
    space = pricedates.index(univdates[1]) - pricedates.index(univdates[0])
    ret = ret.fillna(0)
    price = price.ffill()
    div = div.fillna(0)
    cpr = cpr.ffill()
    result = {'central': {}}
    result['central'][univdates[0]] = 1
    result['peripheral'] = {}
    result['peripheral'][univdates[0]] = 1
    cashvalue = {'central': 0.0, 'peripheral': 0.0}
    stockvalue = {'central': 1.0, 'peripheral': 1.0}
    bar = progressbar.ProgressBar(max_value=len(univdates))
    count = 0
    for t in univdates:
        for j in ['central', 'peripheral']:
            cov = cov_matrix(ret, universes[t][j], window, t)
            pricewindow = price[pricedates.index(t) - 1:pricedates.index(t) + 2 + space][
                universes[t][j]].ffill()
            cprwindow = cpr[cprdates.index(t) - 1:cprdates.index(t) + 2 + space][
                universes[t][j]].ffill()
            divwindow = div[divdates.index(t) - 1:divdates.index(t) + 2 + space][
                universes[t][j]].fillna(0)
            adjustedpricewindow = pricewindow / cprwindow
            r = adjustedpricewindow / adjustedpricewindow.shift(1)
            r = r.iloc[1:]
            if len(np.atleast_1d(cov)) == 1:
                weights = [1]
            elif weighted:
                weights = np.transpose(min_variance_weights(cov)[0])[0]
            else:
                weights = np.divide(np.ones(len(cov)), len(cov))
            cashvalue[j] = 0
            stockvalue[j] = result[j][t]
            for tt in pricedates[pricedates.index(t) + 1:pricedates.index(t) + 1 + space]:
                stockvalue[j] = stockvalue[j] * (weights * r[tt:tt]).sum(axis=1)[0]
                cashvalue[j] += sum(divwindow[tt:tt].values[0] / pricewindow[
                                                                 pricedates[pricedates.index(tt) - 1]:pricedates[
                                                                     pricedates.index(tt) - 1]].values[0] * weights)
                result[j][tt] = stockvalue[j] + cashvalue[j]
        count = count+1
        bar.update(count)
    return result


def benchmark_performance(price, ret, div, cpr, universes, window=100):
    univdates = sorted(universes.keys(), key=lambda d: map(int, d.split('-')))
    pricedates = sorted(pd.to_datetime(price.index.values).strftime("%Y-%m-%d"), key=lambda d: map(int, d.split('-')))
    divdates = sorted(pd.to_datetime(div.index.values).strftime("%Y-%m-%d"), key=lambda d: map(int, d.split('-')))
    cprdates = sorted(pd.to_datetime(cpr.index.values).strftime("%Y-%m-%d"), key=lambda d: map(int, d.split('-')))
    space = pricedates.index(univdates[1]) - pricedates.index(univdates[0])
    ret = ret.fillna(0)
    price = price.ffill()
    div = div.fillna(0)
    cpr = cpr.ffill()
    SP100Performance_weighted = {univdates[0]: 1}
    SP100Performance_unweighted = {univdates[0]: 1}
    cashvalue = {'weighted': 0.0, 'unweighted': 0.0}
    stockvalue = {'weighted': 1.0, 'unweighted': 1.0}
    bar = progressbar.ProgressBar(max_value=len(univdates))
    count = 0
    for t in univdates:
        cov = cov_matrix(ret, ret.keys(), window, t)
        end = int(np.where(price.index == t)[0])
        start = end - window
        univ = price[start:end + 1][price.keys()].columns
        weights = np.transpose(min_variance_weights(cov)[0])[0]
        pricewindow = price[pricedates.index(t) - 1:pricedates.index(t) + 1 + space]
        pricewindow = pricewindow.ffill()
        pricewindow = pricewindow.bfill()
        pricewindow = pricewindow[univ]
        cprwindow = cpr[cprdates.index(t) - 1:cprdates.index(t) + 2 + space][univ].ffill()
        divwindow = div[divdates.index(t) - 1:divdates.index(t) + 2 + space][univ].fillna(0)
        adjustedpricewindow = pricewindow / cprwindow
        r = adjustedpricewindow / adjustedpricewindow.shift(1)
        r = r.iloc[1:]
        cashvalue['weighted'] = 0
        stockvalue['weighted'] = SP100Performance_weighted[t]
        for tt in pricedates[pricedates.index(t) + 1:pricedates.index(t) + 1 + space]:
            stockvalue['weighted'] = stockvalue['weighted'] * (weights * r[tt:tt]).sum(axis=1)[0]
            cashvalue['weighted'] += sum(divwindow[tt:tt].values[0] / pricewindow[
                                                                      pricedates[pricedates.index(tt) - 1]:pricedates[
                                                                          pricedates.index(tt) - 1]].values[
                0] * weights)
            SP100Performance_weighted[tt] = stockvalue['weighted'] + cashvalue['weighted']
        weights = np.divide(np.ones(len(cov)), len(cov))
        cashvalue['unweighted'] = 0
        stockvalue['unweighted'] = SP100Performance_unweighted[t]
        for tt in pricedates[pricedates.index(t) + 1:
                        pricedates.index(t) + 1 + space]:
            stockvalue['unweighted'] = stockvalue['unweighted'] * (weights * r[tt:tt]).sum(axis=1)[0]
            cashvalue['unweighted'] += sum(divwindow[tt:tt].values[0] / pricewindow[
                                                                        pricedates[pricedates.index(tt) - 1]:pricedates[
                                                                            pricedates.index(tt) - 1]].values[
                0] * weights)
            SP100Performance_unweighted[tt] = stockvalue['unweighted'] + cashvalue['unweighted']
        count = count+1
        bar.update(count)

    # df_weighted = pd.DataFrame([[key, value] for key, value in SP100Performance_weighted.iteritems()],
    #                            columns=["Date", "benchmark"])
    # df_weighted = df_weighted.set_index(pd.DatetimeIndex(df_weighted['Date']))
    # df_weighted = df_weighted.drop(['Date'], axis=1)
    # df_weighted.sort_index(inplace=True)
    # df_unweighted = pd.DataFrame([[key, value] for key, value in SP100Performance_unweighted.iteritems()],
    #                              columns=["Date", "benchmark"])
    # df_unweighted = df_unweighted.set_index(pd.DatetimeIndex(df_unweighted['Date']))
    # df_unweighted = df_unweighted.drop(['Date'], axis=1)
    # df_unweighted.sort_index(inplace=True)
    return SP100Performance_weighted, SP100Performance_unweighted


def sharpe_ratio(price, riskless_rate):
    ret = price / price.shift(1)
    ret = ret.iloc[1:] - 1
    mean_ret = np.mean(ret)
    std = np.std(ret)
    return (mean_ret * 252 - riskless_rate) / (std * np.sqrt(252))
