import networkx as nx
import numpy as np
import pandas as pd
from cvxopt import blas, solvers
import cvxopt as opt
from trees import importdata


def weighted_degree_centrality(T):
    """Return a dictionary of weighted degree centrality for each node,
    the weighted degree is defined as (2-gower weight)"""
    for u, v, d in T.edges(data=True):
        d['weight2'] = 2 - d['weight']
    degree = nx.degree(T, weight='weight2')
    degree = dict(degree)
    degree_c = {}
    total = T.size(weight='weight2')
    for i in degree:
        degree_c[i] = degree[i] / total
    return degree_c


def port_change(list1, list2):
    """Calculate the percentage change between two stock lists"""
    count = 0
    if not list1:
        return None
    else:
        for x in list2:
            if x not in list1:
                count += 1.0
    return count / len(list2)


def get_portfolios(dic, c_measure, quantile=0.25):
    """dic: dictionary of MST, which can be generated by function MST()
    returns a dictionary of central and peripheral stock list for each window, the key is enddate of the window.
    apply get_portfolios function to c_measure in ['degree','closeness','betweenness']
    to get three dictionaries for these three measure of centrality."""
    result = {}
    for k, T in dic.items():
        subresult = {}
        central = portfolio(T, c_measure, quantile, "upper")
        peripheral = portfolio(T, c_measure, quantile, "lower")
        subresult["central"] = central
        subresult["peripheral"] = peripheral
        result[k] = subresult
    return result


def portfolio_change(dic):
    """dic: dictonary of central and peripheral stock list for each window, generated from get_portfolio() function
    returns a dataframe of percentage change for stock list"""
    dates = sorted(dic.keys())
    change_dict_c = {}
    change_dict_p = {}
    for i in range(0, len(dates) - 1):
        list1c = dic[dates[i]]["central"]
        list2c = dic[dates[i + 1]]["central"]
        list1p = dic[dates[i]]["peripheral"]
        list2p = dic[dates[i + 1]]["peripheral"]
        change_dict_c[dates[i + 1]] = port_change(list1c, list2c)
        change_dict_p[dates[i + 1]] = port_change(list1p, list2p)
    c = pd.DataFrame(data=change_dict_c, index=["Central"]).T
    p = pd.DataFrame(data=change_dict_p, index=["Peripheral"]).T
    change = pd.concat([c, p], axis=1, join='inner')
    return change


def measure_performance(pricedf, stocklist, startdate, space=1, weights=None):
    start = int(np.where(pricedf.index == startdate)[0])
    end = start + space
    try:
        df = pricedf.iloc[[start, end]][stocklist]
        date = sorted(df.index)[1]
        p_current = df.iloc[0]
        p_next = df.iloc[1]
        a = (p_next / p_current).values
        r = np.average(a, weights=weights)
        return date, r
    except:
        return None, None


def performance(dic, pricedf, space=1, weights=False):
    """dic: dictonary of central and peripheral stock list for each window, generated from get_portfolio() function"""
    colnames = ["Upper", "Lower"]
    if weights:
        colnames.extend([str(x) + '_Weighted' for x in colnames])
        ret = pricedf / pricedf.shift(1)
        ret = ret.iloc[1:]
    colnames.sort()
    dates = sorted(dic.keys())
    performance = pd.DataFrame(columns=colnames)
    for d in dates:
        clist = dic[d]["central"]
        plist = dic[d]["peripheral"]
        date, cr = measure_performance(pricedf, clist, d, space, weights=None)
        pr = measure_performance(pricedf, plist, d, space, weights=None)[1]
        if date:
            performance.set_value(date, "Upper", cr)
            performance.set_value(date, "Lower", pr)
        if weights:
            date, cr = measure_performance(pricedf, clist, d, space, weights=None)
            pr = measure_performance(pricedf, plist, d, space, weights=None)[1]
            performance.set_value(date, "Upper", cr)
            performance.set_value(date, "Lower", pr)
            # use cov_matrix() in clustering_functions.py
            c_cov = cov_matrix(ret, clist, 250, date)
            p_cov = cov_matrix(ret, plist, 250, date)
            cw = min_variance_weights(c_cov)
            pw = min_variance_weights(p_cov)
            crw = measure_performance(pricedf, clist, d, space, weights=cw)[1]
            prw = measure_performance(pricedf, plist, d, space, weights=pw)[1]
            if date:
                performance.set_value(date, "Upper_Weighted", crw)
                performance.set_value(date, "Lower_Weighted", prw)
    performance = performance.sort_index()
    return performance


def clustering_universe(trees, clusterings, c_measure, quantile=0.25):
    """compute the central and peripheral universes according to a given dict of {date: clustering}"""
    result = {}
    sorteddates = sorted(trees.keys(), key=lambda d: map(int, d.split('-')))
    for k in sorteddates:
        T = trees[k]
        subresult = {}
        C = clusterings[k]
        peripheral = []
        central = []
        for c in C:
            if len(list(T.subgraph(c).edges())) == 0:
                # elements in clusters with no edges will be considered peripheral
                peripheral.extend(c)
            else:
                peripheral.extend(portfolio(T.subgraph(c), c_measure, quantile, "lower"))
                central.extend(portfolio(T.subgraph(c), c_measure, quantile, "upper"))
        subresult["central"] = central
        subresult["peripheral"] = peripheral
        result[k] = subresult
    return result


# pick one if there are several stocks with the same highest centrality
def portfolio(MST, c_measure, quantile=0.25, option='upper'):
    """Return a list of the upper or lower 25%(default) of stocks sorted by centrality.
    If the quantile is 0, would return the single one with the highest (lowest) centrality"""
    if c_measure == 'closeness':
        centrality = nx.closeness_centrality(MST, distance="weight")
    elif c_measure == 'degree':
        centrality = weighted_degree_centrality(MST)
    elif c_measure == 'betweenness':
        centrality = nx.betweenness_centrality(MST)
    else:
        print('wrong centrality measure.')
        return None
    items = centrality.items()
    items.sort(key=lambda item: (item[1], item[0]))
    v = [item[1] for item in items]
    sorted_ts = [item[0] for item in items]
    number = int(len(sorted_ts) * quantile)
    if number == 0:
        number = 1
    if option == 'lower':
        pos = len(v) - v[::-1].index(v[number])
        stock_list = sorted_ts[:pos]
    elif option == 'upper':
        if v[-number] == 0:
            pos = len(v) - v[::-1].index(0)
        else:
            pos = v.index(v[-number])
        stock_list = sorted_ts[pos:]
    else:
        print "option must be upper or lower"
        return None
    return stock_list


def clustering_universe(trees, clusterings, c_measure, quantile=0.25):
    """compute the central and peripheral universes according to a given dict of {date: clustering}"""
    result = {}
    sorteddates = sorted(trees.keys(), key=lambda d: map(int, d.split('-')))
    for k in sorteddates:
        T = trees[k]
        subresult = {}
        C = clusterings[k]
        peripheral = []
        central = []
        for c in C:
            if len(list(T.subgraph(c).edges())) == 0:
                # elements in clusters with no edges will be considered peripheral
                peripheral.extend(c)
            else:
                peripheral.extend(portfolio(T.subgraph(c), c_measure, quantile, "lower"))
                central.extend(portfolio(T.subgraph(c), c_measure, quantile, "upper"))
        subresult["central"] = central
        subresult["peripheral"] = peripheral
        result[k] = subresult
    return result


def no_cluster_universe(trees, c_measure, quantile=0.25):
    result = {}
    sorteddates = sorted(trees.keys(), key=lambda d: map(int, d.split('-')))
    for k in sorteddates:
        T = trees[k]
        subresult = {}
        peripheral = []
        central = []
        peripheral.extend(portfolio(T, c_measure, quantile, "lower"))
        central.extend(portfolio(T, c_measure, quantile, "upper"))
        subresult["central"] = central
        subresult["peripheral"] = peripheral
        result[k] = subresult
    return result


def cov_matrix(price, thresh, stocklist, window=250, enddate="2017-02-28", shrinkage='None'):
    """To generate covariance matrix for a certain period and a list of stock (stocklist),
     differs from the one in 'all_functions.py' by the 'stocklist' argument"""
    end = int(np.where(price.index == enddate)[0])
    start = end - window
    sub = price[start:end + 1][stocklist].dropna(thresh=thresh, axis=1)
    sub = sub.ffill()
    sub = sub.bfill()
    subret = sub / sub.shift(1)
    subret = subret.iloc[1:]
    cov_mat = np.cov(subret.T)
    return cov_mat


def min_variance_weights(cov):
    S = opt.matrix(cov)
    n = cov.shape[0]
    q = opt.matrix(0.0, (n, 1))
    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix
    h = opt.matrix(0.0, (n, 1))
    A = opt.matrix(1.0, (1, n))
    b = opt.matrix(1.0)
    solvers.options['show_progress'] = False
    weights = solvers.qp(S, q, G, h, A, b)["x"]
    risk = np.sqrt(blas.dot(weights, S * weights))
    return np.asarray(weights), risk


def clustering_performance(filename, thresh, universes, weighted='TRUE', window=100):
    price = importdata(filename)
    price = price.ffill()
    price = price.bfill()
    univdates = sorted(universes.keys(), key=lambda d: map(int, d.split('-')))
    pricedates = sorted(pd.read_csv(filename)["Date"], key=lambda d: map(int, d.split('-')))
    space = pricedates.index(univdates[1]) - pricedates.index(univdates[0])
    result = {'central': {}}
    result['central'][univdates[0]] = 1
    result['peripheral'] = {}
    result['peripheral'][univdates[0]] = 1
    for t in univdates:
        for j in ['central', 'peripheral']:
            cov = cov_matrix(price, thresh, universes[t][j], window, t)
            pricewindow = price[pricedates.index(t) - 1:pricedates.index(t) + 1 + space][
                universes[t][j]]
            r = pricewindow / pricewindow.shift(1)
            r = r.iloc[1:]
            if len(np.atleast_1d(cov)) == 1:
                weights = [1]
            elif weighted == 'TRUE':
                weights = np.transpose(min_variance_weights(cov)[0])[0]
            else:
                weights = np.divide(np.ones(len(cov)), len(cov))
            for tt in pricedates[pricedates.index(t) + 1:
                            pricedates.index(t) + 1 + space]:
                result[j][tt] = result[j][pricedates[pricedates.index(tt) - 1]] * np.dot(weights,
                                                                                         r[tt:tt].as_matrix()[0])
    return result


def benchmark_performance(filename, thresh, universes, window=100):
    price = importdata(filename)
    univdates = sorted(universes['Newman']['degree'].keys(), key=lambda d: map(int, d.split('-')))
    pricedates = sorted(pd.read_csv(filename)["Date"], key=lambda d: map(int, d.split('-')))
    space = pricedates.index(univdates[1]) - pricedates.index(univdates[0])
    SP100Performance_weighted = {univdates[0]: 1}
    SP100Performance_unweighted = {univdates[0]: 1}
    for t in univdates:
        cov = cov_matrix(price, thresh, price.keys(), window, t)
        end = int(np.where(price.index == t)[0])
        start = end - window
        univ = price[start:end + 1][price.keys()].dropna(thresh=thresh, axis=1).columns
        weights = np.transpose(min_variance_weights(cov)[0])[0]
        pricewindow = price[pricedates.index(t) - window - 1:pricedates.index(t) + 1 + space]
        pricewindow = pricewindow.ffill()
        pricewindow = pricewindow.bfill()
        pricewindow = pricewindow[univ]
        r = pricewindow / pricewindow.shift(1)
        r = r.iloc[1:]
        for tt in pricedates[pricedates.index(t) + 1:
                        pricedates.index(t) + 1 + space]:
            SP100Performance_weighted[tt] = SP100Performance_weighted[pricedates[pricedates.index(tt) - 1]] * np.dot(
                weights, r[tt:tt].as_matrix()[0])
        weights = np.divide(np.ones(len(cov)), len(cov))
        for tt in pricedates[pricedates.index(t) + 1:
                        pricedates.index(t) + 1 + space]:
            # SP100Performance_unweighted[tt] = SP100Performance_unweighted[t]*np.dot(weights,np.divide(price[tt:tt].as_matrix()[0],
            # price[t:t].as_matrix()[0]))
            SP100Performance_unweighted[tt] = SP100Performance_unweighted[
                                                  pricedates[pricedates.index(tt) - 1]] * np.dot(weights,
                                                                                                 r[tt:tt].as_matrix()[
                                                                                                     0])
    df_weighted = pd.DataFrame([[key, value] for key, value in SP100Performance_weighted.iteritems()],
                               columns=["Date", "benchmark"])
    df_weighted = df_weighted.set_index(pd.DatetimeIndex(df_weighted['Date']))
    df_weighted = df_weighted.drop(['Date'], axis=1)
    df_weighted.sort_index(inplace=True)
    df_unweighted = pd.DataFrame([[key, value] for key, value in SP100Performance_unweighted.iteritems()],
                                 columns=["Date", "benchmark"])
    df_unweighted = df_unweighted.set_index(pd.DatetimeIndex(df_unweighted['Date']))
    df_unweighted = df_unweighted.drop(['Date'], axis=1)
    df_unweighted.sort_index(inplace=True)
    return df_weighted, df_unweighted


def sharpe_ratio(price, riskless_rate):
    ret = price / price.shift(1)
    ret = ret.iloc[1:] - 1
    mean_ret = np.mean(ret)
    std = np.std(ret)
    return (mean_ret * 252 - riskless_rate) / (std * np.sqrt(252))
